# https://github.com/googleapis/python-genai

import os
from typing import Dict, Any, List, Literal
from google.genai import Client
from google.genai.types import HttpOptions, GenerateContentConfig, Content, Part
from tech_implement.tools.calculator import Calculator

from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from typing_extensions import TypedDict

GEMINI_MODEL = "gemini-2.0-flash-001"
API_KEY = os.getenv("GOOGLE_API_KEY")

calculator = Calculator()
client = Client(api_key=API_KEY, http_options=HttpOptions(api_version="v1alpha"))


class AgentState(TypedDict):
    """State for the LangGraph agent"""

    messages: List[Dict[str, Any]]
    user_prompt: str
    final_response: str
    iteration_count: int


class LangGraphAgent:
    def __init__(self):
        self.calculator = Calculator()
        self.graph = self._create_graph()

    def _create_graph(self) -> StateGraph:
        """Create the LangGraph workflow"""
        workflow = StateGraph(AgentState)

        # Add nodes
        workflow.add_node("generate_response", self._generate_response)
        workflow.add_node("execute_tools", self._execute_tools)
        workflow.add_node("finalize", self._finalize_response)

        # Add edges
        workflow.set_entry_point("generate_response")

        # Conditional edge from generate_response
        workflow.add_conditional_edges(
            "generate_response",
            self._should_execute_tools,
            {"execute_tools": "execute_tools", "finalize": "finalize"},
        )

        # Edge from execute_tools back to generate_response
        workflow.add_edge("execute_tools", "generate_response")

        # Edge from finalize to END
        workflow.add_edge("finalize", END)

        return workflow.compile()

    def _generate_response(self, state: AgentState) -> AgentState:
        """Generate response from Gemini model"""
        print("ðŸ¤– Generating response from Gemini...")

        # Build content for the request
        if state["iteration_count"] == 0:
            # First iteration - just the user prompt
            contents = [
                Content(role="user", parts=[Part.from_text(text=state["user_prompt"])])
            ]
        else:
            # Subsequent iterations - include previous messages
            contents = []
            for msg in state["messages"]:
                contents.append(msg)

        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=contents,
            config=GenerateContentConfig(tools=[self.calculator.tools]),
        )

        print(
            f"ðŸ¤– Agent response (iteration {state['iteration_count'] + 1}):", response
        )

        # Add the response to messages
        if response.candidates:
            state["messages"].append(response.candidates[0].content)

        state["iteration_count"] += 1

        return state

    def _should_execute_tools(
        self, state: AgentState
    ) -> Literal["execute_tools", "finalize"]:
        """Determine if we need to execute tools or finalize"""
        if not state["messages"]:
            return "finalize"

        last_message = state["messages"][-1]

        # Check if there are function calls in the last message
        if hasattr(last_message, "parts") and last_message.parts:
            for part in last_message.parts:
                if hasattr(part, "function_call") and part.function_call:
                    print("ðŸ”§ Function calls detected, executing tools...")
                    return "execute_tools"

        print("âœ… No function calls detected, finalizing response...")
        return "finalize"

    def _execute_tools(self, state: AgentState) -> AgentState:
        """Execute tool calls and add responses to state"""
        print("ðŸ”§ Executing tools...")

        if not state["messages"]:
            return state

        last_message = state["messages"][-1]
        tool_responses = []

        if hasattr(last_message, "parts") and last_message.parts:
            for part in last_message.parts:
                if hasattr(part, "function_call") and part.function_call:
                    func_call = part.function_call
                    tool_name = func_call.name
                    args = dict(func_call.args)

                    print(f"ðŸ”§ Executing tool: {tool_name} with args: {args}")

                    try:
                        tool_func = getattr(self.calculator, tool_name)
                        tool_result = tool_func(**args)
                        function_response = Part.from_function_response(
                            name=tool_name,
                            response=tool_result,
                        )
                        print(f"âœ… Tool {tool_name} result: {tool_result}")
                    except (AttributeError, TypeError) as e:
                        function_response = Part.from_text(
                            text=f"Error: Tool {tool_name} not found or not callable. {e}"
                        )
                        print(f"âŒ Tool error: {e}")
                    except Exception as e:
                        function_response = Part.from_text(text=f"Error: {str(e)}")
                        print(f"âŒ Execution error: {e}")

                    tool_responses.append(function_response)

        # Add tool responses to messages if any were executed
        if tool_responses:
            # Create a content object with tool responses
            tool_content = Content(role="user", parts=tool_responses)
            state["messages"].append(tool_content)
            print("ðŸ”§ Tool responses added to conversation")

        return state

    def _finalize_response(self, state: AgentState) -> AgentState:
        """Extract final response text"""
        print("âœ… Finalizing response...")

        if state["messages"]:
            # Find the last assistant message with text
            for message in reversed(state["messages"]):
                if hasattr(message, "parts") and message.parts:
                    for part in message.parts:
                        if hasattr(part, "text") and part.text:
                            state["final_response"] = part.text
                            print(f"âœ… Final result: {part.text}")
                            return state

        # Fallback if no text found
        state["final_response"] = "No response generated"
        return state

    def call_agent(self, prompt: str) -> str:
        """Main entry point for the agent"""
        print(f"ðŸš€ Starting LangGraph agent with prompt: {prompt}")

        # Initialize state
        initial_state: AgentState = {
            "messages": [],
            "user_prompt": prompt,
            "final_response": "",
            "iteration_count": 0,
        }

        # Run the graph
        final_state = self.graph.invoke(initial_state)

        return final_state["final_response"]

    def __call__(self, prompt: str) -> str:
        return self.call_agent(prompt)
